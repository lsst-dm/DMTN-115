\section{Introduction} \label{sec:intro}

Object Stores are nothing new to Astronomy.  FITS and IRAF tapes are examples
of object stores that were used when file systems were able to handle the volumes 
of data being produced. Even then, standards for migrating from Object Store to file 
systems were created, allowing for objects to be retrieved into a predefined namespace
on disk.  The explosion of individual POSIX disk capacity and POSIX-like file systems
have produced generations of researchers who have never used an Object Store. While
this growth has supported data systems up till now, the size and complexity of
data being produced by surveys and even pointed telescope archives is reaching
scales where the requirements placed on file access by the POSIX standard are 
significantly hindering our ability to work with data.  Different parallel file systems
have different strengths and weaknesses.  

At large scale, data service providers such as Dropbox and AWS do not store files
in POSIX systems.  Rather they present the illusion of directory structure layered over 
large scale object stores. This allows for faster file access, with only CRUD style
functions taking place on each object.  Further, as the psudo-filesystem layer is simply a 
view of structure typically provided by a graph database, users can arrange or potentially
have real time query driven structure for the file organization, removing how many now 
organize data through a sea of nested symlinks.
Some provide local POSIX caches of that users view of the 
psudo-filesystem, allowing for POSIX style applications to access the files with
fopen, fscan, and fclose standard commands. Additionally, these providers do 
not show users how data are stored. One can simply request data in the format 
needed (e.g. Excel, CSV, or as I assume it is the JSON format the web apps
use for sheets).   At scale, applications often forgo
such posix layers and simply use the CRUD interfaces to the objects to load them into 
memory, act on the objects, and then update or delete them, in the format they need
as input and with the format they natively produce. Further, the data providers need
not update their data archive when formats change, simply provide a new updated
data access format that can be fueled by legacy data formats.

It is time for Astronomy data researchers to follow this curve. As users have migrated
from using tools on their laptops to suport collaboration while reducing
individuals need to manage their systems (Jupyter Hubs, Overleaf, Google Slides), 
so should astronomical data processing and analysis. We propose the adoption of
a common Astronomical data access API layer with the following characteristics:



\section{Recommendation }

How do we paint this as not just VO again

\begin{enumerate}
\item We recommend requiring astronomical data sharing be done through \emph{common API
access layers}.

\item We recommend that all funded astronomical projects with \emph{software deliverables
leverage these APIs} for data access. 

\item We recommend that API layers \emph{expose both data and metadata in common 
transferable and transformable formats} (e.g. CAOM).

\item We recommend community development of \emph{data and metadata transform services}
to support these common API layers 

\item We recommend requiring all data providers to subscribe to a 
\emph{common federated identity service} (e.g. InCommon or 
Globus Auth) that is supported by most university and research organizations. There 
must also be 
an authentication source for users at institutions without such means and for 
citizen science efforts.

\item We recommend that \emph{POSIX based file access be deprecated
in software development} and only used when applications require thread safe
data access (something that is currently not possible with FITS files).

\item We recommend the development of a psudo-directory structure system to
integrate local and remote files into a dynamic namespace for each user and potentially
each users use-case (e.g. the Box sync interface or the FUSE based WholeTale file system
\citep{BRINCKMAN2019854}) 


\end{enumerate}

\section{Why do we recommend these things?}


We should NOT expose file systems and static name spaces.  Data needs to breath. We should use a common abstraction which allows users to work flexibly and not worrying about the data storage beneath.

Filesystems with name spaces are very fragile at large scale. As we get larger data sets we have to trick the filesystem to not run out of Inodes, we make countless sub directories to cope with our thousands of files.
This is turn leads to countless hours spent fighting over how to organize files  the \emph{right way} in a filesystem.
Countless years have been spent fighting over data formats (FITS vs HDF vs CVS vs Pandas).
If we move code then perhaps the filesystem is not organised in the same manner and the code may not work - remote access to allow caching is not always an option.

We need to foster better remote collaboration.  The laptop is the bane of file sharing.
This has changed with cloud based psudo-file systems but require storage in a single 
cloud providers infrastructure. By creating a FileSystem as a Service (FSAAS) federated
across data and cloud providers, we will win.


\section{What should  we do ?}
Users should not care and repositories should not be tied to legacy formats  and storage representations because of legacy constraints  at other repositories.
The rest of the world has already moved on,  Google, Amazon, Git, netflix etc. do not host large filesystems and and can scale because they are not limited by this antiquated formalism.

Everyone hates object stores  yet this is what underlies the big silicon valley outfits - we should expose a layer which finds appropriate data using metadata queries and returns that data formatted for the applications requesting it.
.
Meta data -- how do we treat it what is it ..  tagging?  Translations...make this part of the data agent (aka butler)
Dynamic data structuring (more than just here is your dict of data and metadata..but transformations into API consumable pieces).  Knows how to consume...knows how to export in different ways

This "Infrastructure as Code" \citep{morris2016infrastructure} approach lowers the bar to entry
and allows for easier adoption of more standardized services that will enable large-scale
astronomical research in ways that are well demonstrated in plant genomics (CyVerse and Galaxy), natural hazards (Designsafe), and surface water research (Hydroshare).


\section{Example Service Architecture- The Butler}
Unpack butler as an example â€¦

\section{Library example - AstroPy}
What is needed to go beyond LSST and AstroPy?


