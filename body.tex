\section{Introduction} \label{sec:intro}

{\bf Executive summary:} the filesystem notion limits our ability to scale processing and has long since been dropped by industry. Astronomy needs to move on.

Object Stores are nothing new to Astronomy.  \gls{FITS} and \gls{IRAF} tapes are examples
of object stores that were used when file systems were unable to handle the volumes
of data being produced. Even then, standards for migrating from \gls{Object} Store to file
systems were created, allowing for objects to be retrieved into a predefined namespace
on disk.  The explosion of individual \gls{POSIX} disk capacity and \gls{POSIX}-like file systems
have produced generations of researchers who have never used an \gls{Object} Store. While
this growth has supported data systems up till now, the size and complexity of
data being produced by surveys and even pointed telescope archives is reaching
scales where the requirements placed on file access by the \gls{POSIX} standard are
significantly hindering our ability to work with data.  Different parallel file systems
have different strengths and weaknesses.

At large scale, data service providers such as Dropbox and \gls{AWS} do not store files
in \gls{POSIX} systems.  Rather they present the illusion of directory structure layered over
large scale object stores. This allows for faster file access, with only \gls{CRUD} style
functions taking place on each object.  Further, as the pseudo-filesystem layer is simply a
view of structure typically provided by a graph database, users can arrange or potentially
have real time query driven structure for the file organization, removing how many now
organize data through a sea of nested symlinks.
Some provide local \gls{POSIX} caches of that users view of the
pseudo-filesystem, allowing for \gls{POSIX} style applications to access the files with
fopen, fscan, and fclose standard commands. Additionally, these providers do
not show users how data are stored. One can simply request data in the format
needed (e.g. Excel, \gls{CSV}, or as I assume it is the \gls{JSON} format the web apps
use for sheets).   At scale, applications often forgo
such Posix layers and simply use the \gls{CRUD} interfaces to the objects to load them into
memory, act on the objects, and then update or delete them, in the format they need
as input and with the format they naively produce. Further, the data providers need
not update their data archive when formats change, simply provide a new updated
data access format that can be fueled by legacy data formats.

It is time for Astronomy data researchers to follow this curve. As users have migrated
from using tools on their laptops to support collaboration while reducing
individuals need to manage their systems (Jupyter Hubs, Overleaf, Google Slides),
so should astronomical data processing and analysis. We propose the adoption of
a common Astronomical data access \gls{API} layer.


\section{Recommendation }
We need to adopt an industry  standard layered architecture for astronomy processing and data access as
depicted in \figref{fig:ci}. Part os such a model is an object store oriented API - this should be used for data sharing.


\begin{figure}
\centering
\includegraphics[width=0.45\textwidth]{CI}
\includegraphics[width=0.45\textwidth]{CI-LSST}
\caption{Industry standard Cyber infrastructure model (left) and an astronomy instantiation of such a model (right)\label{fig:ci}}
\end{figure}

All funded funded astronomical projects with \emph{software deliverables leverage these APIs} for data access.
Of course this is not quite enough - the  \gls{API} layers must \emph{expose both data and \gls{metadata} in common
transferable and transformable formats} (e.g. \gls{CAOM}.

The astronomical community must develop \emph{data and \gls{metadata} transform services}
to support these common \gls{API} layers to aid interoperation. To further aid ease of use and cut down on wasted effort
all data providers should  to subscribe to a
\emph{common federated identity service} (e.g. InCommon or
Globus Auth) that is supported by most university and research organizations. There
must also be
an authentication source for users at institutions without such means and for
citizen science efforts.

This would imply that \emph{POSIX based file access be deprecated
in software development} and only used when applications require thread safe
data access (something that is currently not possible with \gls{FITS} files).

We should furthermore  develop a pseudo-directory structure system to
integrate local and remote files into a dynamic namespace for each user and potentially
each users use-case (e.g. the Box sync interface or the \gls{FUSE} based WholeTale file system
\citep{BRINCKMAN2019854})

\section{Why we should kill the filesystem}

Users should not care and repositories should not be tied to legacy formats  and storage representations because of legacy constraints  at other repositories.
The rest of the world has already moved on,  Google, Amazon, Git, netflix etc. do not host large filesystems and and can scale because they are not limited by this antiquated formalism.


Filesystems with name spaces are very fragile at large scale. As we get larger data sets we have to trick the filesystem to not run out of Inodes, we make countless sub directories to cope with our thousands of files.
This is turn leads to countless hours spent fighting over how to organize files  the \emph{right way} in a filesystem.
Countless years have been spent fighting over data formats (\gls{FITS} vs \gls{HDF} vs \gls{CSV} vs Pandas).
If we move code then perhaps the filesystem is not organised in the same manner and the code may not work - remote access to allow caching is not always an option.

We need to foster better remote collaboration.  The laptop is the bane of file sharing.
This has changed with cloud based pseudo-file systems but require storage in a single
cloud providers infrastructure. By creating a Filesystem as a Service (\gls{FSAAS}) federated
across data and cloud providers, we will win.


This "Infrastructure as Code" \citep{morris2016infrastructure} approach lowers the bar to entry
and allows for easier adoption of more standardized services that will enable large-scale
astronomical research in ways that are well demonstrated in plant genomics (CyVerse and Galaxy), natural hazards (Designsafe), and surface water research (Hydroshare).

\section{Example Service Architecture- The Butler}
The LSST butler

\section{Library example - AstroPy}
What is needed to go beyond \gls{LSST} and AstroPy?


